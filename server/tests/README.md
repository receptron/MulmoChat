# LLM API Test Suite

This directory contains test applications for the text generation and function calling capabilities of all supported LLM providers.

## Test Categories

### Text Generation Tests

Basic text generation tests that validate the provider integration:

- `test-text-openai.ts` - OpenAI GPT models
- `test-text-anthropic.ts` - Anthropic Claude models
- `test-text-google.ts` - Google Gemini models
- `test-text-ollama.ts` - Ollama local models

### Function Calling Tests

Advanced tests that validate tool/function calling capabilities:

- `test-tools-openai.ts` - OpenAI function calling (✓ Full support)
- `test-tools-anthropic.ts` - Anthropic tool use (✓ Full support)
- `test-tools-google.ts` - Google function calling (✓ Full support)
- `test-tools-ollama.ts` - Ollama function calling (⚠ Model-dependent)

## Running Tests

### Prerequisites

1. Start the development server:
   ```bash
   npm run server
   ```

2. Set up API keys in `.env`:
   ```bash
   OPENAI_API_KEY=sk-...
   ANTHROPIC_API_KEY=sk-ant-...
   GEMINI_API_KEY=...
   ```

### Run Text Generation Tests

```bash
# OpenAI
npm run test:text:openai

# Anthropic
npm run test:text:anthropic

# Google
npm run test:text:google

# Ollama (no API key required)
npm run test:text:ollama
```

### Run Function Calling Tests

```bash
# OpenAI (recommended - most reliable)
npm run test:tools:openai

# Anthropic
npm run test:tools:anthropic

# Google
npm run test:tools:google

# Ollama (requires compatible model)
npm run test:tools:ollama
```

## Function Calling Test Flow

Each function calling test follows this pattern:

1. **Initial Request**: Send a user message that requires tool usage
2. **Tool Call Detection**: Verify the model returns tool call requests
3. **Tool Execution**: Execute the requested tools locally (mocked)
4. **Results Submission**: Send tool results back to the model
5. **Final Response**: Model generates natural language response using tool outputs

### Example Tools

The tests use two example functions:

**get_weather**
- Gets weather for a location
- Parameters: `location` (required), `unit` (optional)

**calculate**
- Performs basic math operations
- Parameters: `operation`, `a`, `b` (all required)

## Provider-Specific Notes

### OpenAI
- **Status**: ✓ Full support, most reliable
- **Format**: Uses OpenAI's native function calling format
- **Tool IDs**: Generated by the API
- **Best for**: Production use

### Anthropic (Claude)
- **Status**: ✓ Full support with tool_use blocks
- **Format**: Uses Anthropic's native tool use format
- **Tool IDs**: Generated by the API
- **Tool Results**: Sent as `tool_result` content blocks
- **Best for**: Complex reasoning tasks

### Google (Gemini)
- **Status**: ✓ Full support with function declarations
- **Format**: Uses Gemini's `functionCall` format
- **Tool IDs**: Uses function name (not generated ID)
- **Tool Results**: Sent as `functionResponse` parts
- **Best for**: Fast inference with function calling

### Ollama
- **Status**: ⚠ Model-dependent support
- **Format**: OpenAI-compatible format
- **Models**: Only newer models support function calling:
  - ✓ `llama3.1:8b` and above
  - ✓ `qwen2.5:7b` and above
  - ✓ `mistral-nemo:12b` and above
  - ✗ Older models may not support tools
- **Best for**: Local/private deployments with compatible models

## Troubleshooting

### "No tool calls received"

**OpenAI/Anthropic/Google**: This indicates a bug - these providers should reliably call tools when appropriate.

**Ollama**: This is expected for models without function calling support. Try:
```bash
# Use a model known to support function calling
OLLAMA_TEST_MODEL=llama3.1:8b npm run test:tools:ollama
```

### "Request failed: 404"

The server is not running. Start it with:
```bash
npm run server
```

### "API_KEY is required"

Set the appropriate environment variable in `.env`:
- OpenAI: `OPENAI_API_KEY`
- Anthropic: `ANTHROPIC_API_KEY`
- Google: `GEMINI_API_KEY`

### Ollama connection errors

1. Ensure Ollama is running:
   ```bash
   ollama serve
   ```

2. Verify the model is installed:
   ```bash
   ollama list
   ```

3. Pull the model if needed:
   ```bash
   ollama pull llama3.1
   ```

## Custom Test Prompts

All tests accept custom prompts via command line:

```bash
# Text generation with custom prompt
npm run test:text:openai "Explain quantum computing"

# Function calling with custom prompt
npm run test:tools:openai "What's the weather in Tokyo? Also calculate 123 + 456"
```

## Environment Variables

### Server Configuration
- `TEST_SERVER_URL` - Default: `http://localhost:3001`

### Model Selection
- `OPENAI_TEST_MODEL` - Default: `gpt-4o-mini`
- `ANTHROPIC_TEST_MODEL` - Default: `claude-3-5-sonnet-latest`
- `GEMINI_TEST_MODEL` - Default: `gemini-2.5-flash`
- `OLLAMA_TEST_MODEL` - Default: `gpt-oss:20b`

### Ollama Configuration
- `OLLAMA_BASE_URL` - Default: `http://127.0.0.1:11434`

## Implementation Details

### Provider Updates

All providers have been updated to support function calling:

1. **server/llm/providers/openai.ts**
   - Native OpenAI function calling format
   - Tool calls in assistant messages
   - Tool results with `tool_call_id`

2. **server/llm/providers/anthropic.ts**
   - Anthropic `tool_use` and `tool_result` blocks
   - Tool definitions as `input_schema`
   - Multi-part content blocks

3. **server/llm/providers/google.ts**
   - Gemini `functionCall` and `functionResponse` parts
   - Function declarations in tools array
   - Tool results use function name as ID

4. **server/llm/providers/ollama.ts**
   - OpenAI-compatible tool format
   - Model-dependent support
   - Graceful degradation for unsupported models

### API Endpoints

The tests use the `/api/text/generate` endpoint with the following format:

```typescript
{
  provider: "openai" | "anthropic" | "google" | "ollama",
  model: string,
  messages: TextMessage[],
  tools?: ToolDefinition[],
  maxTokens?: number
}
```

## Next Steps

After verifying function calling works:

1. Use the session-based API for stateful conversations
2. Implement custom tools for your application
3. Build agentic workflows with tool chaining
4. Monitor token usage and costs

## Contributing

When adding new tests:

1. Follow the existing pattern (initial request → tool execution → final response)
2. Include clear console output for each step
3. Handle errors gracefully
4. Document provider-specific quirks
