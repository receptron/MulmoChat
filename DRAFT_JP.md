# The Dawn of the AI-Native Operating System

ChatGPTがOpenAIから発表されて3年が経とうとしています。その後、Function Call、MCP、Code Interpreter、Artifact、Multi-modal LLM、Agentなど、数多くの仕組みが生まれました。しかし、ChatGPTの基本的なインターフェイスはいまだに「テキストベースのチャット」に留まっています。

私たちソフトウェア・エンジニアの間で、Claude Codeのようなチャット形式のプログラミング環境が広く使われている事実は、LLMと人間の間の**自然言語インターフェイス（Natural Language Interface）**が、既存のGUIを超えるポテンシャルを持つことを示しています。

一方、MicrosoftのCoPilotのように、従来のアプリケーション（Excelなど）に自然言語インターフェイスを後付けする試みは、必ずしも成功していません。それは、これらのアプリケーションが「LLMが存在しない時代の設計思想」に基づいているからです。

同じ理由で、LLMにブラウザや既存アプリを操作させるアプローチにも限界があります。真にAIネイティブなアプローチとは、アプリケーションの機能をAPIとしてLLMに提示し、ユーザーの意図（Intent）を一連のAPIコールに変換することです。LLMが、ユーザーとアプリケーションの間のインタープリタの役割を果たすのです。

つまり、自然言語を理解するLLMの存在を前提としたAI-NativeなコンピュータのUser Experienceは、ユーザーとLLMの間のNLUIと、ユーザーとアプリケーションの間のGUIを融合したユーザー体験を提供すべきであり、アプリケーションのアーキテクチャそのものも従来とは大きく異なるものである必要があるのです。

本稿では、「AI-Nativeなコンピュータ（OS）のUser Experienceとアプリケーション・アーキテクチャ」という視点から、業界全体が向かうべき方向性を示すために設計された**MulmoChat（Multi-modal Chat）**というオープンソースのプロトタイプを紹介します。

## NLUIとGUIの融合

Function CallとMCP（以後はTool、もしくはTool Callと呼びます）は、LLMに外部機能を提供する仕組みとして登場しました。これらは、JSON形式のパラメータを受け取り、JSON形式の結果を返すという、テキストベースのインターフェイスです。

Tool Callの結果として、アプリケーション独自のウィンドウを表示したり、ブラウザーで特定のページを開くことは可能ですが、それらはチャットインターフェイスとは切り離されており、「NLUI（Natural Language UI）とGUIの融合」とは呼べません。

この問題を解決するために、MulmoChatではTool Callの仕組みを拡張し、NLUIとGUIの自然な統合を実現しました。具体的には、Tool Callのreturn valueを拡張し、LLMへの返答に加えて、ユーザーに表示すべきTool-specificなデータを返すことを可能にしています。このデータの表示は、システムに登録されたTool専用のビューアーが担当します。

このアーキテクチャにより、アプリケーションはチャット（NLUI）の流れの中で、独自のインタラクティブなGUIをリアルタイムに生成・操作できるようになります。

以下は、Excelに相当する表計算アプリケーションを、このアーキテクチャ上で動作させた場合の一例です。


1. 「SpreadSheet」アプリケーションは、LLMが呼び出せるtoolと、tool-specificなデータを表示するビューアーを登録する。
2. ユーザーが「Shows me the present value of $1000 monthly income over a year, making it easy to change the discount rate.」と指示を与える。
3. LLMはユーザーの意図（intent）を解釈し、"SpreadSheet" toolを呼び出すためのJSONデータを構築する。
4. Toolは、LLMに対して「表計算データを生成・表示中である」と通知し、システムに対しては生成したスプレッドシートデータを返す。
5. システムは返されたデータタイプを判定し、対応するビューアーを起動する。
6. ビューアーが生成されたスプレッドシートをユーザーに表示する。
7. ユーザーはGUI上で直接操作を行い、その結果が再びチャットコンテキスト（NLUI）にフィードバックされる。

このように、MulmoChatでは、LLM・GUI・ユーザーの三者が同一の文脈で連続的にやりとりできます。これこそが、NLUIとGUIの真の融合であり、AI-Nativeなコンピューティング環境における新しいユーザー体験の核となります。

![](https://mag2.thelifeisbeautiful.com/Nov2025/discount2.png)

## Beyond the Sea of App Icons

上のシナリオは、「ユーザー体験」という観点から、既存のシステムと大きく異なります。

多くのiPhoneユーザーは、数十から百個を超えるアプリケーションをインストールしています。そのため、何かをするたびに「無数のアプリ・アイコンの海の中から、必要なものを探し出す」という煩雑な作業を繰り返さなければなりません。
AI-Nativeなシステムでは、どのToolを使うかの判断はLLMが行います。

ユーザーは「アプリを選んで起動する」という作業から完全に解放されるのです。

表計算やプレゼン資料、文書作成などは、LLMが利用できるToolとしてシステムに登録されているだけで、ユーザーは「アプリケーション」という概念そのものから自由になります。

さらに、スプレッドシートやプレゼン資料のドラフトは、LLMが自動的に生成してくれるため、Learning Curve（学習コスト）は桁違いに低くなります。

下の図は、MulmoChatに対してユーザーが“Make a travel guide for Tokyo with pictures of 3 famous landmarks.”と指示した際の出力例です。Wordに相当するToolが呼び出され、わずか数十秒でドキュメントが生成されています。

![](https://mag2.thelifeisbeautiful.com/Nov2025/tokyo_guide.png)

ユーザーはこのドキュメントを起点に、情報を追記したり、LLMにさらなる指示を出して内容を更新できます。これにより、チャットがそのままドキュメント生成プロセスになるのです。

この仕組みは、ClaudeのArtifactやChatGPTのCanvasに似ていますが、二つの点で大きく異なります。

1. Extensibility（拡張性）

　ArtifactやCanvasは限定されたデータタイプにしか対応していませんが、MulmoChatの仕組みはサードパーティが新しいデータタイプやビューアーを自由に追加できます。

2. Communication-first（対話中心）

　ArtifactやCanvasは成果物の生成を目的としていますが、MulmoChatはLLMとユーザーのコミュニケーションを中心に設計されています。生成されるドキュメントやソフトウェアは、あくまでその対話の一部にすぎません。

この設計により、たとえば旅行代理店が顧客向け資料を作るだけでなく、LLMが“旅行代理店そのもの”としてユーザーに提案を行うことが可能になります。

さらに、この仕組みはLLMがユーザーから情報を得る場面にも使えます。下の例は、MulmoCast内で「病院のレセプション」として動作するロールです。LLMはユーザーから情報を得るためのフォームをダイナミックに生成し、自然な対話の流れで提示します。

![](https://mag2.thelifeisbeautiful.com/Nov2025/Receptionist.png)

このように、MulmoChatでは情報の生成と収集が、同じ自然言語インターフェイス上でシームレスに行われる。これこそが、AI-Nativeコンピューティング環境における新しい「ユーザー体験の完成形」です。

## Domain-Specific Presentation Language — The Bridge Between Intent and Interface

このアーキテクチャにおいて極めて重要な役割を果たすのが、**LLMがユーザーに情報（入力フォームを含む）を提示する際に生成するDSL（Domain-Specific Language）**です。より正確には、表示に特化した言語であることから、**Domain-Specific Presentation Language（DSPL）**と呼ぶのが適切かもしれません。

スプレッドシート、ドキュメント、フォームなど、それぞれのToolが必要とするデータ形式は異なります。各Toolはそのスキーマを定義としてLLMに提示しており、LLMはユーザーのリクエストに応じてそのスキーマに準拠したデータを生成し、Toolを呼び出します。

多くの場合、Toolはその生成データをそのままシステムにTool-specificなデータとして渡し、専用のビューアーがそのデータを画面に描画し、ユーザーとのやり取りを担います。

この過程では、以下の二つの変換が行われています。

1. LLM: ユーザーの意図（intent）をDSLへ変換する。
2. Viewer: DSLをGUIへ変換する。

つまり、DSLと専用ビューアーの組み合わせこそが、テキスト（JSON）生成を得意とするLLMにインタラクティブなGUI表現を可能にしているのです。

DSLとしては、ドキュメント作成に使うMarkdownをはじめ、HTML、SVG、TeXなどの汎用的でオープンな形式を採用することも、特定のTool専用の独自DSLを用いることも可能です。

汎用的な形式はLLMがすでに学習しているため出力精度が高い一方で、構文が冗長でトークン数が増えやすく、期待通りの表現を得にくい場合もあります。そのため、MulmoChatでは目的に応じて汎用DSLと専用DSLを柔軟に使い分ける設計としています。

LLM開発の潮流は、Tool Callとコード生成能力の強化に向かっていますが、DSL活用はこの流れと極めて相性が良いのです。実際、MulmoChatの開発を通じて、GPT-5やSonnet 4.5といった最先端モデルだけでなく、gpt-oss:20bやqwen3:30bのような中小規模モデルでも十分な性能が発揮されることが確認されています。

DSLは、LLMとGUIの間をつなぐ“共通言語”であり、MulmoChatのAI-Nativeアーキテクチャの哲学的中核です。

