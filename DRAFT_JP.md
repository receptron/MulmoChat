# The Dawn of the AI-Native Operating System

ChatGPTがOpenAIから発表されて3年が経とうとしています。その後、Function Call、MCP、Code Interpreter、Artifact、Multi-modal LLM、Agentなど、数多くの仕組みが生まれました。しかし、ChatGPTの基本的なインターフェイスはいまだに「テキストベースのチャット」に留まっています。

私たちソフトウェア・エンジニアの間で、Claude Codeのようなチャット形式のプログラミング環境が広く使われている事実は、LLMと人間の間の**自然言語インターフェイス（Natural Language Interface）**が、既存のGUIを超えるポテンシャルを持つことを示しています。

一方、MicrosoftのCoPilotのように、従来のアプリケーション（Excelなど）に自然言語インターフェイスを後付けする試みは、必ずしも成功していません。それは、これらのアプリケーションが「LLMが存在しない時代の設計思想」に基づいているからです。

同じ理由で、LLMにブラウザや既存アプリを操作させるアプローチにも限界があります。真にAIネイティブなアプローチとは、アプリケーションの機能をAPIとしてLLMに提示し、ユーザーの意図（Intent）を一連のAPIコールに変換することです。LLMが、ユーザーとアプリケーションの間のインタープリタの役割を果たすのです。

つまり、自然言語を理解するLLMの存在を前提としたAI-NativeなコンピュータのUser Experienceは、ユーザーとLLMの間のNLUIと、ユーザーとアプリケーションの間のGUIを融合したユーザー体験を提供すべきであり、アプリケーションのアーキテクチャそのものも従来とは大きく異なるものである必要があるのです。

本稿では、「AI-Nativeなコンピュータ（OS）のUser Experienceとアプリケーション・アーキテクチャ」という視点から、業界全体が向かうべき方向性を示すために設計された**MulmoChat（Multi-modal Chat）**というオープンソースのプロトタイプを紹介します。

## NLUIとGUIの融合

Function CallとMCP（以後はTool、もしくはTool Callと呼びます）は、LLMに外部機能を提供する仕組みとして登場しました。これらは、JSON形式のパラメータを受け取り、JSON形式の結果を返すという、テキストベースのインターフェイスです。

Tool Callの結果として、アプリケーション独自のウィンドウを表示したり、ブラウザーで特定のページを開くことは可能ですが、それらはチャットインターフェイスとは切り離されており、「NLUI（Natural Language UI）とGUIの融合」とは呼べません。

この問題を解決するために、MulmoChatではTool Callの仕組みを拡張し、NLUIとGUIの自然な統合を実現しました。具体的には、Tool Callのreturn valueを拡張し、LLMへの返答に加えて、ユーザーに表示すべきTool-specificなデータを返すことを可能にしています。このデータの表示は、システムに登録されたTool専用のビューアーが担当します。

このアーキテクチャにより、アプリケーションはチャット（NLUI）の流れの中で、独自のインタラクティブなGUIをリアルタイムに生成・操作できるようになります。

以下は、Excelに相当する表計算アプリケーションを、このアーキテクチャ上で動作させた場合の一例です。


1. 「SpreadSheet」アプリケーションは、LLMが呼び出せるtoolと、tool-specificなデータを表示するビューアーを登録する。
2. ユーザーが「Shows me the present value of $1000 monthly income over a year, making it easy to change the discount rate.」と指示を与える。
3. LLMはユーザーの意図（intent）を解釈し、"SpreadSheet" toolを呼び出すためのJSONデータを構築する。
4. Toolは、LLMに対して「表計算データを生成・表示中である」と通知し、システムに対しては生成したスプレッドシートデータを返す。
5. システムは返されたデータタイプを判定し、対応するビューアーを起動する。
6. ビューアーが生成されたスプレッドシートをユーザーに表示する。
7. ユーザーはGUI上で直接操作を行い、その結果が再びチャットコンテキスト（NLUI）にフィードバックされる。

このように、MulmoChatでは、LLM・GUI・ユーザーの三者が同一の文脈で連続的にやりとりできます。これこそが、NLUIとGUIの真の融合であり、AI-Nativeなコンピューティング環境における新しいユーザー体験の核となります。

![](https://mag2.thelifeisbeautiful.com/Nov2025/discount2.png)

## No more "See of Application Icons"

上のシナリオは、「ユーザー体験」という観点から、既存のシステムと一つの大きな違いがあります。

多くのiPhoneユーザーは、数十から百個を超えるアプリケーションをインストールしているため、何かをやる際には、「たくさんのアプリ・アイコンの中から必要なものを選び出す」という煩雑な作業を毎回しなければなりません。

AI-Nativeなシステムにおいては、どのToolを使うかの判断は、LLMが行うため、ユーザーは「立ち上げるアプリケーションを選ぶ」という煩雑な作業から解放されるのです。

それどころか、表計算、プレゼン資料作成、文章作成、などはLLMが使うことが出来る「Tool」としてシステムに登録されるだけで、ユーザーは「アプリケーション」という観念から解放されるのです。

AI-Nativeなシステムにおいては、スプレッドシートを作る、プレゼン資料を作るなどの作業も、少なくとも最初のドラフトはLLMが作ってくれるため、Learning Curveが桁違いに低くなります。

下の図は、MulmoChatに対してユーザーが"Make a travel guide for Tokyo with pictures of 3 famous landmarks"と入力した際に、Wordに相当するToolによってドキュメントが作られる様子を表示しています。

![](https://mag2.thelifeisbeautiful.com/Nov2025/tokyo_guide.png)

LLMの力を最大限に活用しているからこそ、こんなドキュメントを僅か数十秒で作ることができるし、これをベースにユーザーが情報を追加したり、LLMに更なる指示を出して変更することも可能です。

この仕組みは、ClaudeのArtifact、ChatGPTのCanvasと似ていますが、二つの点で大きく違います。

一つ目は、この仕組みがExtensibleである点です。特定のデータタイプしか扱えないArtifactやCanvasと違って、サードパーティがさまざまなデータタイプとビューアーをシステムに追加することが可能です。

二つ目は、これがLLMとユーザーの間のコミュニケーションの一環である点です。ArtifactやCanvasは、ドキュメントやプログラムを作ることを目的に設計されたものですが、この仕組みの１番の目的は、LLMとユーザーとの間のコミュニケーションにあります。

もちろん、このシステムにおいても、トラベルエージェントが「顧客向けの資料」を作ることに使うことは可能ですが、プライマリな使い方は、LLMがトラベルエージェントの代わりに、ユーザーに対して旅行のアイデアを提案するシナリオです。

高性能なLLMの誕生により、ドキュメントやソフトウェアの生成コストが桁違いに下がりましたが、それによって可能になるのは、この例のように、LLMがユーザーに情報を伝えるために「必要に応じてオンデマンドで」ドキュメントやソフトウェアを作ることです。

ちなみに、この仕組みは、LLMがユーザーから情報を得る際にも使うことが可能です。下の例は、MulmoCast内に作られた「病院のレセプション」というロールと、ユーザーのやり取りを示したものです。このシナリオにおいては、LLMはユーザーから情報を得るためのフォームをダイナミックに生成し、ユーザーに提示しています。

![](https://mag2.thelifeisbeautiful.com/Nov2025/Receptionist.png)

## Role of Domain-specific Presentation Data

##



